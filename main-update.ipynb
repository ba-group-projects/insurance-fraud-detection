{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score,roc_auc_score,roc_curve\n",
    "import statsmodels.api as sm\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "### Question\n",
    "Briefly discuss why it is more difficult to find a good classifier on such a dataset than on one where, for example, 5,000 claims are fraudulent, and 5,000 are not. In particular, consider what happens when undetected fraudulent claims are very costly to the insurance company.\n",
    "\n",
    "### Answer\n",
    "When the dataset is highly unbalanced, seen with the car-insurance data, the machine learning algorithm will accurately predict the majority class but poorly predict in minority class. Due to the feature of cross-entropy the algorithm tends to label the minority into majority if we do not adjust the threshold, which is 0.5 by default. In our scenario, the algorithm tends to predict all the claims as non-fraudulent which has significant financial implications if the fraud case is mislabeled.However, the wrong prediction will increase the false-negative rate, which will subsequently increase the cost of fraudulent claims.\n",
    "\n",
    "Another issue with scarce minority data is that individual, or a combination of variables, that have a high probability of being fraudulent may be overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "### Question\n",
    "Load the dataset \"Insurance_claims.csv\" and clean it as appropriate for use with machine learning algorithms. A description of the features can be found at the end of this document.\n",
    "\n",
    "### Principle\n",
    "1. Since the dataset is highly unbalanced, and the fraudulent dataset is very scarce, we should not drop the data labelled 'fraudulent'.\n",
    "2. When the variables are dummy variables, we tend to keep the NaN value as a classification value rather than drop it.\n",
    "3. When the variables are numeric variables, we will check how many NaN values are related to the rows that are fraudulent. If there are few of them, we will drop the variable. Otherwise, we will find a way to fill the missing values.\n",
    "\n",
    "### Proprocess methodology\n",
    "1. load data and select useful columns\n",
    "2. drop duplicates\n",
    "3. drop unreasonable rows\n",
    "4. fix NaN and missing values data\n",
    "    - fix NaN data for categorical variables\n",
    "    - fix NaN data for numeric variables\n",
    "5. reformat date data\n",
    "6. check the distribution of dataset with statistical analysis\n",
    "    - check whether there is a significant distribution difference in every column in fraud data and non-fraud data\n",
    "    - plot categorical data\n",
    "7. clean features\n",
    "    - dummy variables\n",
    "      - PolicyholderOccupation\n",
    "      - ClaimCause\n",
    "      - ClaimInvolvedCovers\n",
    "      - DamageImportance\n",
    "      - FirstPartyVehicleType \n",
    "      - ConnectionBetweenParties\n",
    "      - PolicyWasSubscribedOnInternet\n",
    "    - extract features in 'ClaimInvolvedCovers'\n",
    "8. split data and scale\n",
    "9. show the data structure after preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data and select useful columns\n",
    "We are going to use the following relevant features to predict the fraud cases as they are relevant and easy to quantify:\n",
    "1. PolicyholderOccupation\n",
    "2. LossDate\n",
    "3. FirstPolicySubscriptionDate\n",
    "4. ClainType\n",
    "5. ClaimInvolvedCovers\n",
    "6. DamageImportance\n",
    "7. FirstPartyVehicleType\n",
    "8. ConnectionBetweenParties\n",
    "9. PolicyWasSubscribedOnInternet\n",
    "10. NumberOfPoliciesOfPolicyholder\n",
    "11. FpVehicleAgeMonths\n",
    "12. EasinessToStage\n",
    "13. ClaimWihoutIdentifiedThirdParty\n",
    "14. ClaimAmount\n",
    "15. LossHour\n",
    "16. PolicyHolderAge\n",
    "17. NumberOfBodilyInjuries\n",
    "18. FirstPartyLiability\n",
    "19. LossAndHolderPostCodeSame\n",
    "\n",
    "And we also need label:\n",
    "1. Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and get a brief idea of the data\n",
    "df = pd.read_csv('./materials/Insurance_claims.csv')\n",
    "# get useful features that needed in the machine learning model\n",
    "# TODO using nlp to insurer notes data\n",
    "needed_columns = [ 'PolicyholderOccupation',\n",
    "       'LossDate', 'FirstPolicySubscriptionDate', 'ClaimCause',\n",
    "       'ClaimInvolvedCovers', 'DamageImportance', 'FirstPartyVehicleType',\n",
    "       'ConnectionBetweenParties', 'PolicyWasSubscribedOnInternet',\n",
    "       'NumberOfPoliciesOfPolicyholder', 'FpVehicleAgeMonths',\n",
    "       'EasinessToStage', 'ClaimWihoutIdentifiedThirdParty', 'ClaimAmount',\n",
    "       'LossHour', 'PolicyHolderAge', 'NumberOfBodilyInjuries',\n",
    "       'FirstPartyLiability', 'LossAndHolderPostCodeSame','Fraud']\n",
    "df = df[needed_columns]\n",
    "\n",
    "# show the first 5 rows, get some idea of the data structure\n",
    "print(f'Data sample:')\n",
    "print(df.head(5)) #TODO use sentiment analysis \n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# get the columns name\n",
    "print('Data Columns:')\n",
    "print(str(df.columns))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# get some basic information about the data, and we found that the min number \n",
    "# of FpVehicleAgeMonths is less than 0, which does't make sense. We are going \n",
    "# to detect whether these rows are fraud cases or not. If they are all non-fraud,\n",
    "# we can drop the rows with negative FpVehicleAgeMonths value. Otherwise, we will \n",
    "# create a new feature that record these abnormal rows since these unreasonable \n",
    "# values might be evidence of fraud cases.\n",
    "print('Data description:')\n",
    "print(df.describe())\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# check whether there are any duplicated rows and we found there are 8 duplicated rows,\n",
    "# which we are going to drop.\n",
    "print('Data duplicated rows:')\n",
    "print(df.duplicated().sum())\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Drop the duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the data before dropping duplicated rows:')\n",
    "df_shape_before_drop = df.shape\n",
    "print(df.shape)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# drop the duplicated rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print('The shape of the data after dropping duplicated rows:')\n",
    "df_shape_after_drop = df.shape\n",
    "print(df.shape)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "print(f'The number of rows that are dropped: {df_shape_before_drop[0]-df_shape_after_drop[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Drop unreasonable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether unreasonable rows contain fraud cases\n",
    "df_unreasonable_rows = df[df['FpVehicleAgeMonths'] < 0]\n",
    "df_shape_before_drop = df_unreasonable_rows.shape\n",
    "print(df_unreasonable_rows)\n",
    "print('-----------------------------------------------------')\n",
    "# we can find that these three rows are not fraud cases. \n",
    "# Since we have enough non-fraud data, we can drop these rows.\n",
    "df_shape_before_drop = df.shape\n",
    "df.drop(df_unreasonable_rows.index, inplace=True)\n",
    "df_shape_after_drop = df.shape\n",
    "\n",
    "print(f'The number of rows that are dropped: {df_shape_before_drop[0]-df_shape_after_drop[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Deal with NaN data\n",
    "Check how many NaN values are in each column.\n",
    "\n",
    "We can find that except for 'FirstPartyVehicleNumber', 'ThirdPartyVehicleNumber', and 'InsurerNotes', which might not be used in the models, most the NaN values are concentrated in 'PolicyholderOccupation', and 'ClaimCause', which are categorical variables. In this case, these NaN values will be converted into a category value in order to account for the influence of the missing variables, regardless of why they are missing.\n",
    "\n",
    "Concerning the numeric variables, we will check how many of them are missing when the claim is fraudulent or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much NaN values in each column.\n",
    "print(f'Number of NaN values in each column:') \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that in the Fraud case, there is a lot of data missing in categorical variables, but few in numeric variables.\n",
    "\n",
    "As a result, we can set NaN as a category of categorical data and generate dummy variables. Finally, we will drop the rows that contain NaN values in numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing data when Frand is True\n",
    "df_fraud = df[df[\"Fraud\"]==1]\n",
    "print(f'Number of NaN values in each column when Frand is True:') \n",
    "print(df_fraud.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_frand = df[df[\"Fraud\"]==0]\n",
    "print(f'Number of NaN values in each column when Frand is False:')\n",
    "print(df_non_frand.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing data in categorical columns with string NaN and make it a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = ['PolicyholderOccupation', 'ClaimCause','ClaimInvolvedCovers', 'DamageImportance', 'FirstPartyVehicleType','ConnectionBetweenParties', 'PolicyWasSubscribedOnInternet']\n",
    "df[dummy_columns] = df[dummy_columns].fillna('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the missing data in numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape_before_drop = df.shape\n",
    "df_fraud_shape_before_drop = df[df[\"Fraud\"]==1].shape\n",
    "df.dropna(subset=[\"LossHour\",\"PolicyHolderAge\",\"FpVehicleAgeMonths\"],inplace=True)\n",
    "df_shape_after_drop = df.shape\n",
    "df_fraud_shape_after_drop = df[df[\"Fraud\"]==1].shape\n",
    "print(f'The number of rows that are dropped: {df_shape_before_drop[0]-df_shape_after_drop[0]}')\n",
    "print(f'The number of rows that are dropped when Frand is True: {df_fraud_shape_before_drop[0]-df_fraud_shape_after_drop[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these steps, we don't have any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of NaN values in each column:')\n",
    "df.isna().sum()\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "print('The shape of final datasets:')\n",
    "print(df.shape)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "print(\"Data sample:\")\n",
    "print(df.head(5))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Reformat date data\n",
    "We tend to consider the date data as an important feature considering people might have different tendencies to commit fraud, depending on the time period. This argument is supported by Pascal Blanque (2002), who asserted that the economic crisis is a significant reason to commit fraud.\n",
    "\n",
    "However, the data is presented in datetime format, which cannot be used in machine learning models, so it will be converted into a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the date into timestamp\n",
    "df['LossDate'] = df['LossDate'].apply(lambda x:datetime.datetime.strptime(x,'%d.%M.%y').timestamp())\n",
    "df['FirstPolicySubscriptionDate'] = df['FirstPolicySubscriptionDate'].apply(lambda x:datetime.datetime.strptime(x,'%d.%M.%y').timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Check the distribution of dataset with statistical analysis\n",
    "Based on the distribution of the numerical data, we can see that there are some differences between the fraud and non-fraud cases, especially in the distribution of 'FpVehicleAgeMonths' and 'claimAmount'. The fraud cases tend to have a more dispersed distribution in 'FpVehicleAgeMonths' and higher value in 'claimAmount'. \n",
    "\n",
    "In the categorical data some interesting patterns emerged. For example, in the fraud cases, the percentage of 'TotalLoss' is much higher than the non-fraud cases, which is understandable. When comparing fraud and non-fraud, fraud claims tended to have a high percentage of the same address details, whether it be email addresses, bank accounts, or phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datasets that are fraud cases and non-fraud cases\n",
    "df_fraud = df[df['Fraud'] == 1]\n",
    "df_non_fraud = df[df['Fraud'] == 0]\n",
    "\n",
    "# get the mean of both datasets\n",
    "print('The mean of fraud datasets:')\n",
    "print('-----------------------------------------------------')\n",
    "df_fraud_mean = df_fraud.mean()\n",
    "print(df_fraud_mean)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "print('The mean of non-fraud datasets:')\n",
    "print('-----------------------------------------------------')\n",
    "df_non_fraud_mean = df_non_fraud.mean()\n",
    "print(df_non_fraud_mean)\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of fraud datasets\n",
    "print('The distribution numerical data of fraud datasets:')\n",
    "df_fraud.iloc[:,:-1].hist(bins=50, figsize=(20,15),density=True, xlabelsize=10, ylabelsize=10) #TODO add title\n",
    "plt.show()\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# plot the distribution of non-fraud datasets\n",
    "print('The distribution numerical data of non-fraud datasets:')\n",
    "df_non_fraud.iloc[:,:-1].hist(bins=50, figsize=(20,15),density=True, xlabelsize=10, ylabelsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = ['PolicyholderOccupation', 'ClaimCause', 'DamageImportance', 'FirstPartyVehicleType','ConnectionBetweenParties', 'PolicyWasSubscribedOnInternet']\n",
    "\n",
    "names = locals()\n",
    "for i, col in enumerate(dummy_columns):\n",
    "    names[f\"ax_{i}\"] = df.groupby(['Fraud'])[col].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True, figsize=(20,10),title=f\"Fig {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Clean features\n",
    "### 2.7.1 Deal with outliers \n",
    "The presence of outliers in the data is a major problem for machine learning algorithms (Chakravarty, et al., 2020).\n",
    "\n",
    "Based on the histogram plots, the most prominent outliers were seen from ClaimAmount and FpVehicleAgeMonths for non-fraud cases. We decided to retain these data points because we would want to train the model to detect these rare cases correctly since we assume that fraudsters are very unlikely to claim huge amounts for a very old car to blend in with non-fraud cases.\n",
    "     \n",
    "### 2.7.2 Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variables for categorical data\n",
    "dummy_columns = ['PolicyholderOccupation', 'ClaimCause', 'DamageImportance', 'FirstPartyVehicleType','ConnectionBetweenParties', 'PolicyWasSubscribedOnInternet']\n",
    "# Dummy variables for categorical data\n",
    "df = pd.get_dummies(df,columns=dummy_columns,drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ClaimInvolvedCovers data\n",
    "# Get all covers\n",
    "all_unique =  df[\"ClaimInvolvedCovers\"].unique().tolist()\n",
    "all_covers = str.join(' ', all_unique) # join the string to list\n",
    "all_covers_set = set(all_covers.split()) # use set to drop duplicate covers\n",
    "print(all_covers_set)\n",
    "\n",
    "for cover in all_covers_set:\n",
    "    df[f\"ClaimInvolvedCovers_{cover}\"] = df[\"ClaimInvolvedCovers\"].apply(lambda x: 1 if cover in x else 0)\n",
    "df = df.drop(columns=['ClaimInvolvedCovers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Split the data and scale\n",
    "We are going to use MinMaxScaler in this case, since we can find the dataset does not follow a Gaussian distribution. In addition, we will fit the MinMaxScaler model with training and validation dataset and apply the model to the test dataset to ensure we are blind to the data information before we test the data.\n",
    "\n",
    "### 2.8.1 Process of splitting and scaling data:\n",
    "1. Turn the dataframe into X and y array\n",
    "1. Split the train and test\n",
    "2. Fit the MinMaxScaler to train data and then apply the model to test data\n",
    "3. Split the train data into train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dataframe into X and y array\n",
    "X = df.drop(['Fraud'],axis=1,inplace=False).to_numpy()\n",
    "y = df[['Fraud']].to_numpy().flatten()\n",
    "print('The shape of X:')\n",
    "print(X.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('The shape of y:')\n",
    "print(y.shape)\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=192,test_size=0.2)\n",
    "\n",
    "# Fit the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split the train datasets into train and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,random_state=192,test_size=0.25)\n",
    "\n",
    "## Show the data structure of the split datasets\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of X_train:')\n",
    "print(X_train.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of y_train:')\n",
    "print(y_train.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of X_val:')\n",
    "print(X_val.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of y_val:')\n",
    "print(y_val.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of X_test:')\n",
    "print(X_test.shape)\n",
    "print('-----------------------------------------------------')\n",
    "print('Shape of y_test:')\n",
    "print(y_test.shape)\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "### Question\n",
    "Start by creating a (deep) neural network in TensorFlow and train it on the data. Using training and validation sets, find a model with high accuracy, then evaluate it on the test set. In particular, record both the accuracy and AUC. Briefly discuss what issues you observe based on the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf ./logs100/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Set the range of hyperparameters\n",
    "We are going to explore the performance of distinct neural network when training with different hyperparameters using the following methodology:\n",
    "1. Learning rate:\n",
    "    Usually, a lower learning rate will result in a better fit model and higher learning rate will accelerate the training process but result in a underfitting model. In this case, we set the range of learning rate from 10**(0.001) to 10**(0.1)\n",
    "2. Optimizer:\n",
    "    We are going to try both AdamOptimizer and sgd optimizer. In most cases, SGD sacrifices efficiency for better convergence quality.\n",
    "3. Dropout:\n",
    "    We are going to use  HP_DROPOUT to set the dropout rate and HP_WHETHER_DROPOUT to decide whether to drop. The dropout rate ranges from 0.1 to 0.3 to see how the performance of the model changes.\n",
    "4. Number of neurons in the hidden layer:\n",
    "    In each hidden layer, we are going to use the same number of neurons. In terms of the number of neurons, we are going to use a rule of thumb, in which we can calculate the number of neurons as:\n",
    "    \n",
    "    $N_h = \\frac{N_i}{\\alpha * (N_i+N_o))})$\n",
    "    \n",
    "    Ni = number of input neurons.\n",
    "    \n",
    "    No = number of output neurons.\n",
    "    \n",
    "    Ns = number of samples in training data set.\n",
    "    \n",
    "    α = an arbitrary scaling factor usually 2-10.\n",
    "\n",
    "    In our case, we are going to set $\\alpha$ randomly to 2,3,4\n",
    "5. Number of hidden layers:\n",
    "    Normally, if the model is very simple, one hidden layer is enough according to Reed and Marks argument (Reed & Marks, 1999).\n",
    "    ```\n",
    "    Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more? One reason hangs on the words “sufficiently large”. Although a single hidden layer is optimal for some functions, there are others for which a single-hidden-layer-solution is very inefficient compared to solutions with more layers.\n",
    "    ``` \n",
    "\n",
    "    However, in terms of a complex model, Goodfellow et al. (2016) argued that: \n",
    "    ```\n",
    "    Specifically, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.\n",
    "    ——Deep learning, 2016\n",
    "    ```\n",
    "    Since our model might not be able to be explained by a linear function, we are going to set the range of hidden layers from 1 to 3.\n",
    "6. Activation\n",
    "    We are going to compare the performance of the following activation functions:\n",
    "    1. sigmoid\n",
    "    2. relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001,0.1))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_WHETHER_DROPOUT = hp.HParam('whether_dropout', hp.Discrete([True, False]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.3))\n",
    "# the number of units in the hidden layer, 1 time, 2 times or 3 times of the unit number of input layer\n",
    "BASE_NUM_UNITS = X_train.shape[0]/(X_train.shape[1] + 1)\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([int(BASE_NUM_UNITS/2), int(BASE_NUM_UNITS/3), int(BASE_NUM_UNITS/4),int(BASE_NUM_UNITS/5)])) \n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'sigmoid']))\n",
    "HP_HIDDEN_LAYER_NUMBER = hp.HParam('hidden_layer_number', hp.Discrete(range(1,4)))\n",
    "METRIC_CROSSENTROPY = 'binary_crossentropy'\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have set up our parameters and metrics, we write those into our folder with the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer('logs100/hparam_tuning').as_default():\n",
    "    hp.hparams_config(hparams=[HP_LEARNING_RATE, HP_OPTIMIZER, HP_DROPOUT, HP_NUM_UNITS,HP_ACTIVATION,HP_HIDDEN_LAYER_NUMBER],\n",
    "                      metrics = [hp.Metric(METRIC_CROSSENTROPY, display_name='CROSSENTROPY')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hparams,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(192)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # set patience to 10 to accelerate the training\n",
    "    if hparams[HP_WHETHER_DROPOUT] == True:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "            tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=hparams[HP_ACTIVATION])]*hparams[HP_HIDDEN_LAYER_NUMBER]+[\n",
    "            tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=hparams[HP_ACTIVATION])]*hparams[HP_HIDDEN_LAYER_NUMBER]+[\n",
    "            tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "    if hparams[HP_OPTIMIZER] == 'sgd':\n",
    "        # Note that exploding gradients can be a big problem when running regressions, especially under SGD\n",
    "        # Hence, we use \"gradient clipping\" with parameter alpha, which means that the gradients are manually kept between -1 and 1\n",
    "        # This is of course another hyperparameter that we might tune!\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=hparams[HP_LEARNING_RATE], clipvalue=1)\n",
    "    elif hparams[HP_OPTIMIZER] == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    # random_seed = 192\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_val,y_val) ,callbacks=[early_stopping_cb],)\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    x_test_predict = model.predict(X_test)\n",
    "    # calculate the roc\n",
    "    roc_score = roc_auc_score(y_test, x_test_predict)\n",
    "    # calculate the accuracy suppose the threshold is 0.5\n",
    "    x_test_predict_binary = np.where(x_test_predict>0.5,1,0)\n",
    "    accuracy = accuracy_score(y_test, x_test_predict_binary)\n",
    "    # calculate the sensitivity\n",
    "    sensitivity = recall_score(y_test, x_test_predict_binary)\n",
    "    return loss, accuracy,roc_score,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        \n",
    "        loss, accuracy,roc_score,sensitivity = train_model(hparams) #TODO whether I did it right\n",
    "        tf.summary.scalar('ACCUARY', accuracy, step=1)\n",
    "        tf.summary.scalar('LOSS', loss, step=1)\n",
    "        tf.summary.scalar('ROC', roc_score, step=1)\n",
    "        tf.summary.scalar('SENSITIVITY', sensitivity, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train the model and view on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "\n",
    "# 100 total sessions\n",
    "total_sessions = 100 #FIXME: change this to the number of sessions you want to run, and fix the issue in the metrics\n",
    "\n",
    "for session in range(total_sessions):\n",
    "    \n",
    "    # Create hyperparameters randomly\n",
    "    whether_dropout = HP_WHETHER_DROPOUT.domain.sample_uniform()\n",
    "    dropout_rate = HP_DROPOUT.domain.sample_uniform()\n",
    "    num_units = HP_NUM_UNITS.domain.sample_uniform()\n",
    "    optimizer = HP_OPTIMIZER.domain.sample_uniform()\n",
    "    activation = HP_ACTIVATION.domain.sample_uniform()\n",
    "    hidden_layer_number = HP_HIDDEN_LAYER_NUMBER.domain.sample_uniform()\n",
    "    \n",
    "    \n",
    "    r = -3*np.random.rand()\n",
    "    learning_rate = 10.0**r\n",
    "    \n",
    "    # Create a dictionary of hyperparameters\n",
    "    hparams = { HP_LEARNING_RATE: learning_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_WHETHER_DROPOUT: whether_dropout,\n",
    "                HP_DROPOUT: dropout_rate,\n",
    "                HP_NUM_UNITS: num_units,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_HIDDEN_LAYER_NUMBER: hidden_layer_number}\n",
    "    \n",
    "    # train the model with the chosen parameters\n",
    "    run_name = \"run-%d\" % session\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run('logs100/hparam_tuning/' + run_name, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evalaute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "\n",
    "dropout = 0.12292\n",
    "learning_rate = 0.01\n",
    "number_units = 34\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(number_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(number_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # set patience to 10 to accelerate the training\n",
    "# log = model.fit(X_train, y_train, epochs=100, validation_data =(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "log = model.fit(X_train, y_train, epochs=25, validation_data =(X_val, y_val))\n",
    "\n",
    "model.save('./models/question3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(log):\n",
    "    # plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "    plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "    # plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "    plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, y_test)\n",
    "y_test_predict = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the roc\n",
    "roc_score = roc_auc_score(y_test, y_test_predict)\n",
    "print(f\"test_loss: {test_loss}\")\n",
    "print(f\"roc: {roc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(fn,fp):\n",
    "    return 10*fn+fp\n",
    "\n",
    "cost_lost = {}\n",
    "for i in np.linspace(0,0.1,501):\n",
    "    pred_y = np.where(y_test_predict.flatten()> i, 1, 0)\n",
    "    cm = confusion_matrix(y_test,pred_y)\n",
    "    fn,fp = cm[1][0],cm[0][1]\n",
    "    # cost_lost[\"Threshold: \"+str(i)] = calculate_cost(fn,fp)\n",
    "    cost_lost[i] = calculate_cost(fn,fp)\n",
    "\n",
    "optimal_threshold = min(cost_lost, key=cost_lost.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.where(y_test_predict > optimal_threshold, 1, 0)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "print(f\"Confusion matrix is :\" )\n",
    "print(cm)\n",
    "accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "print(f\"Accuracy rate is {accuracy_rate}\")\n",
    "# calculate the sensitivity \n",
    "sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "print(f\"Sensitivity is {sensitivity}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Briefly discuss what issues you observe based on the metrics\n",
    "\n",
    "The dataset is too imbalanced for the model to be able to learn the relationship between the features and the target variable because the `loss` does not decrease.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "### Question\n",
    "The file \"SMOTE.ipynb\" explains the process in detail and shows how to change the dataset with an example. You can copy and adjust the code to make it work within your analysis. You can adjust the \"sampling_strategy\" parameters as you see fit, particularly if\n",
    "you want to fine-tune your model in part 5.\n",
    "\n",
    "### Principle\n",
    "In this part, we are going to try both oversampling and undersampling.\n",
    "\n",
    "Here is the procedure to process the data\n",
    "1. Split the raw data into train and test\n",
    "2. Fit the MinMaxScaler to train data and then apply the model to test data\n",
    "3. Oversample and undersample the train data\n",
    "4. Split the train data into train and validation data\n",
    "5. Show the data distribution before and after oversampling and undersampling\n",
    "\n",
    "The reason to do so is that we want to make sure that the training and validation data are similar. When we test our model, we tend to use real test data instead of the simulated test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Split the raw data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=192,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fit the MinMaxScaler to train data and apply the scaler to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Oversample and undersample the train data\n",
    "We will successively try to oversample the minority class to 10%, 30%, 50% of the size of all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "# k_neighbors set to 20 to make sure that the result is more general\n",
    "over = imblearn.over_sampling.SMOTE(sampling_strategy=0.1, random_state = 483, k_neighbors=20)  \n",
    "X_over_synth_10, y_over_synth_10 = over.fit_resample(X_train, y_train)\n",
    "over = imblearn.over_sampling.SMOTE(sampling_strategy=0.5, random_state = 483, k_neighbors=20)\n",
    "X_over_synth_30, y_over_synth_30 = over.fit_resample(X_train, y_train)\n",
    "over = imblearn.over_sampling.SMOTE(sampling_strategy=1, random_state = 483, k_neighbors=20)\n",
    "X_over_synth_50, y_over_synth_50 = over.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Percentage of 1 in y_over_synth10:\", Counter(y_over_synth_10)[1]/len(y_over_synth_10))\n",
    "print(\"Percentage of 1 in y_over_synth30:\", Counter(y_over_synth_30)[1]/len(y_over_synth_30))\n",
    "print(\"Percentage of 1 in y_over_synth50:\", Counter(y_over_synth_50)[1]/len(y_over_synth_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will successively try to undersample the minority class to 10%, 30%,50% of the size of majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=0.1, random_state = 483)  \n",
    "X_under_synth_10, y_under_synth_10 = under.fit_resample(X_train, y_train)\n",
    "under = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=0.5, random_state = 483)\n",
    "X_under_synth_30, y_under_synth_30 = under.fit_resample(X_train, y_train)\n",
    "under = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=1, random_state = 483)\n",
    "X_under_synth_50, y_under_synth_50 = under.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Percentage of 1 in y_under_synth_10:\", Counter(y_under_synth_10)[1]/len(y_under_synth_10))\n",
    "print(\"Percentage of 1 in y_under_synth_30:\", Counter(y_under_synth_30)[1]/len(y_under_synth_30))\n",
    "print(\"Percentage of 1 in y_under_synth_50:\", Counter(y_under_synth_50)[1]/len(y_under_synth_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to illustrate before (left plot) and after (right plot) undersampling of majority class. Green dots represent the minority class, and Grey dots represent the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot xticks\n",
    "x = np.arange(0, len(X_under_synth_10), 1)\n",
    "plt.xticks(x, x)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_train[:, 2], X_train[:, 3], c=y_train, s=10, cmap=\"Accent_r\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_under_synth_10[:, 2], X_under_synth_10[:, 3], c=y_under_synth_10, s=10, cmap=\"Accent_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Split the train data into train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over_synth_10_train, X_over_synth_10_val, y_over_synth_10_train, y_over_synth_10_val = train_test_split(\n",
    "    X_over_synth_10, y_over_synth_10, random_state=192, test_size=0.25)\n",
    "X_over_synth_30_train, X_over_synth_30_val, y_over_synth_30_train, y_over_synth_30_val = train_test_split(\n",
    "    X_over_synth_30, y_over_synth_30, random_state=192, test_size=0.25)\n",
    "X_over_synth_50_train, X_over_synth_50_val, y_over_synth_50_train, y_over_synth_50_val = train_test_split(\n",
    "    X_over_synth_50, y_over_synth_50, random_state=192, test_size=0.25)\n",
    "X_under_synth_10_train, X_under_synth_10_val, y_under_synth_10_train, y_under_synth_10_val = train_test_split(\n",
    "    X_under_synth_10, y_under_synth_10, random_state=192, test_size=0.25)\n",
    "X_under_synth_30_train, X_under_synth_30_val, y_under_synth_30_train, y_under_synth_30_val = train_test_split(\n",
    "    X_under_synth_30, y_under_synth_30, random_state=192, test_size=0.25)\n",
    "X_under_synth_50_train, X_under_synth_50_val, y_under_synth_50_train, y_under_synth_50_val = train_test_split(\n",
    "    X_under_synth_50, y_under_synth_50, random_state=192, test_size=0.25)\n",
    "\n",
    "print(\"The length of X_over_synth_10_train is:\", len(X_over_synth_10_train))\n",
    "print(\"The length of X_over_synth_10_val is:\", len(X_over_synth_10_val))\n",
    "print(\"The length of X_over_synth_30_train is:\", len(X_over_synth_30_train))\n",
    "print(\"The length of X_over_synth_30_val is:\", len(X_over_synth_30_val))\n",
    "print(\"The length of X_over_synth_50_train is:\", len(X_over_synth_50_train))\n",
    "print(\"The length of X_over_synth_50_val is:\", len(X_over_synth_50_val))\n",
    "print(\"The length of X_under_synth_10_train is:\", len(X_under_synth_10_train))\n",
    "print(\"The length of X_under_synth_10_val is:\", len(X_under_synth_10_val))\n",
    "print(\"The length of X_under_synth_30_train is:\", len(X_under_synth_30_train))\n",
    "print(\"The length of X_under_synth_30_val is:\", len(X_under_synth_30_val))\n",
    "print(\"The length of X_under_synth_50_train is:\", len(X_under_synth_50_train))\n",
    "print(\"The length of X_under_synth_50_val is:\", len(X_under_synth_50_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "### Question\n",
    " Create a new (deep) neural network and train it on your enhanced dataset. Use training and validation sets derived from the enhanced dataset to find a model with high accuracy. Evaluate your final model on a test set consisting only of original data. Again, record the accuracy and AUC. Briefly discuss the changes you would expect in the metrics and the actual changes you observe. Would you say that you are now doing better at identifying fraudulent claims?\n",
    "\n",
    "### Methodology:\n",
    "1. Test all synthetic datasets on basic model to select specific SMOTE technique\n",
    "2. Selected 10% oversampling\n",
    "3. Use tuner to train model and pick best model\n",
    "4. Plot training and validation loss\n",
    "5. Evaluate model using test set\n",
    "6. Pick optimal threshold using cost = 10*FN + FP\n",
    "7. Plot ROC curve and confusion matrix heatmap\n",
    "\n",
    "### Principle\n",
    "To simplify the problem, and save computational time, we will train the synthetic data to a very simple neural network, and then compare the performance of this distinct synthetic data.\n",
    "After doing this, we are going to select the best performing dataset and then we can use a tuner to train the model for it.\n",
    "\n",
    "The neural network structure is as follows:\n",
    "1. Input layer\n",
    "2. 2 hidden layers, in which the number of neurons in each layer is equal to 60 and 'relu' function is used\n",
    "3. No dropout layer\n",
    "4. Output layer, with 'sigmoid' activation function.\n",
    "5. Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Use a controlled model to compare the performance of the different sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    def __init__(self, X_train, y_train, X_val=None, y_val=None,X_test=None,y_test=None, epochs=100,early_stopping:bool=False,patience:int=10):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(192)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping= early_stopping\n",
    "        self.patience = patience\n",
    "        self.simple_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(60, activation='relu'),\n",
    "            tf.keras.layers.Dense(60, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def compile(self):\n",
    "        self.simple_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self): # We fit the model with train and validation data becasue validation data can tell us when to stop training\n",
    "        if self.early_stopping:\n",
    "            early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=self.patience, restore_best_weights=True) # set patience to 10 to accelerate the training\n",
    "            self.log = self.simple_model.fit(self.X_train, self.y_train,validation_data= (self.X_val, self.y_val),callbacks=[early_stopping_cb], epochs=self.epochs)\n",
    "        else:\n",
    "            self.log = self.simple_model.fit(self.X_train, self.y_train,validation_data= (self.X_val, self.y_val),epochs=self.epochs)\n",
    "\n",
    "    def evaluate(self): # evalute it on the test dataset, since we are going to predict the raw data like test one\n",
    "        loss = self.simple_model.evaluate(self.X_test, self.y_test)\n",
    "        x_test_predict = self.simple_model.predict(self.X_test)\n",
    "        # calculate the roc\n",
    "        roc_score = roc_auc_score(self.y_test, x_test_predict)\n",
    "        # calculate the accuracy suppose the threshold is 0.5\n",
    "        x_test_predict_binary = np.where(x_test_predict>0.5,1,0)\n",
    "        accuracy = accuracy_score(self.y_test, x_test_predict_binary)\n",
    "        # calculate the sensitivity\n",
    "        sensitivity = recall_score(self.y_test, x_test_predict_binary)\n",
    "        return {'loss': loss, 'accuracy': accuracy, 'sensitivity': sensitivity, 'roc': roc_score,'modle':self.simple_model}\n",
    "    \n",
    "    def draw_the_loss_curve(self):\n",
    "        # plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "        plt.plot(self.log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "        # plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "        plt.plot(self.log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def run(self):\n",
    "        self.compile()\n",
    "        self.fit()\n",
    "        # self.draw_the_loss_curve()\n",
    "        return self.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if we apply the model to the test data (real data), and select ROC as the metrics to select the sampling strategy, we can find that in the oversampling strategy, the ROC rate is higher than undersampling strategy. Since there is a slight difference between about 10%, 30% and 50% oversampling, we will use the 10% oversampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "res_dict = {}\n",
    "for X_train,y_train,X_val,y_val,name in zip([X_over_synth_10_train,X_over_synth_30_train,X_over_synth_50_train,X_under_synth_10_train,X_under_synth_30_train,X_under_synth_50_train], \n",
    "                           [y_over_synth_10_train,y_over_synth_30_train,y_over_synth_50_train,y_under_synth_10_train,y_under_synth_30_train,y_under_synth_50_train],\n",
    "                           [X_over_synth_10_val,X_over_synth_30_val,X_over_synth_50_val,X_under_synth_10_val,X_under_synth_30_val,X_under_synth_50_val],\n",
    "                           [y_over_synth_10_val,y_over_synth_30_val,y_over_synth_50_val,y_under_synth_10_val,y_under_synth_30_val,y_under_synth_50_val],\n",
    "                           [\"over_synth_10\",\"over_synth_30\",\"over_synth_50\",\"under_synth_10\",\"under_synth_30\",\"under_synth_50\"]):\n",
    "    tm = TrainModel(X_train,y_train,X_val, y_val, X_test = X_test, y_test =y_test,early_stopping=True)\n",
    "    res = tm.run()\n",
    "    res_dict[name] = (res)\n",
    "\n",
    "for key,value in res_dict.items():\n",
    "    print(f\"{key}:{value['roc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Train model using Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hp):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(192)\n",
    "    number_units = hp.Int('number_units', min_value=20, max_value=80, step=20)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
    "    # optim_algo = hp.Choice('optimizer', values=['sgd','adam']) \n",
    "    optim_algo = 'adam'\n",
    "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=1, sampling='log') \n",
    "    number_layers = hp.Int('number_layers', min_value=1, max_value=3) # hidden layers\n",
    "    activation = hp.Choice('activation', values=['relu','sigmoid'])\n",
    "    whether_dropout = hp.Choice('whether_dropout', values=[True,False])\n",
    "\n",
    "    if whether_dropout == True:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(number_units, activation=activation)]*number_layers+[ # number_layers is the number of hidden layers\n",
    "            tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(number_units, activation=activation)]*number_layers+[ # number_layers is the number of hidden layers\n",
    "            tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "\n",
    "    if optim_algo== 'sgd':\n",
    "        # Note that exploding gradients can be a big problem when running regressions, especially under SGD\n",
    "        # Hence, we use \"gradient clipping\" with parameter alpha, which means that the gradients are manually kept between -1 and 1\n",
    "        # This is of course another hyperparameter that we might tune!\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate, clipvalue=1)\n",
    "    elif optim_algo == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "    # random_seed = 192\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm log file\n",
    "# ! rm -rf ./logs_over_synth_10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the best model for X_over_synth_10_train\n",
    "tuner = kt.Hyperband(train_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='logs_over_synth_10',\n",
    "                     project_name='kt_tutorial_over_synth_10')\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "tuner.search(X_over_synth_10_train, y_over_synth_10_train, epochs=10, validation_data =(X_over_synth_10_val,y_over_synth_10_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # set patience to 10 to accelerate the training\n",
    "log = best_model.fit(X_over_synth_10_train, y_over_synth_10_train, epochs=100, validation_data =(X_over_synth_10_val,y_over_synth_10_val),callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(log):\n",
    "    # plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "    plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "    # plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "    plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model in question 5\n",
    "# best_model.save('./best_model_question_5.h5')\n",
    "\n",
    "# load the best model in question 5\n",
    "best_model = tf.keras.models.load_model('./models/best_model_question_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test dataset, and we can find that our ROC score is about 0.82 which is good. In the next part, we are going to select the threshold to predict the fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = best_model.evaluate(X_test, y_test)\n",
    "y_test_predict = best_model.predict(X_test).flatten()\n",
    "# calculate the roc\n",
    "roc_score = roc_auc_score(y_test, y_test_predict)\n",
    "print(f\"test_loss: {loss}\")\n",
    "print(f\"roc: {roc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is more costly to miss the fraud cases, and less costly to make a false alarm, we are going to suppose that the cost missed fraud is 10 times more than the cost of false alarm (This ratio can be adjusted to the real case).\n",
    "Thus, our cost function is:\n",
    "\n",
    "```Cost = 10*FN + FP```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_threshold is 0.0936\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(fn,fp):\n",
    "    return 10*fn+fp\n",
    "\n",
    "cost_lost = {}\n",
    "for i in np.linspace(0,0.1,501):\n",
    "    pred_y = np.where(y_test_predict.flatten()> i, 1, 0)\n",
    "    cm = confusion_matrix(y_test,pred_y)\n",
    "    fn,fp = cm[1][0],cm[0][1]\n",
    "    # cost_lost[\"Threshold: \"+str(i)] = calculate_cost(fn,fp)\n",
    "    cost_lost[i] = calculate_cost(fn,fp)\n",
    "\n",
    "optimal_threshold = min(cost_lost, key=cost_lost.get)\n",
    "print(f\"optimal_threshold is {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.where(y_test_predict > optimal_threshold, 1, 0)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "print(f\"Confusion matrix is :\" )\n",
    "print(cm)\n",
    "accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "print(f\"Accuracy rate is {accuracy_rate}\")\n",
    "# calculate the sensitivity \n",
    "sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "print(f\"Sensitivity is {sensitivity}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Briefly discuss the changes you would expect in the metrics and the actual changes you observe. Would you say that you are now doing better at identifying fraudulent claims?\n",
    "\n",
    "Utilizing the same cost function, we can find that the sensitivity score is higher than the imbalanced dataset. We are performing better at identifying fraudulent claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "### Question\n",
    "Our second approach will be to use an autoencoder to learn what \"normal\" (non-fraudulent) data \"looks like.\"\n",
    "\n",
    "1. Prepare dataset for autoencoder\n",
    "\n",
    "   - Using the original data, create a training set that contains only non fraudulent claims\n",
    "   - As well as validation and test sets that contain non fraudulent and fraudulent claims. \n",
    "   - Make sure to spread fraudulent claims evenly across validation and test sets.\n",
    "\n",
    "2. Create an autoencoder using TensorFlow\n",
    "\n",
    "   - Ensure that the middle hidden layer has fewer neurons than your input features. \n",
    "   - Use training and validation sets to find a model that represents its input data well. In particular, you will want to predict your validation set observations. \n",
    "   - For each observation, you can measure the difference between the original observations and the predicted one, using, for example, the mean squared error of all features of the observation. \n",
    "   - Plot the errors for all your validation set observations in a histogram - in a good model, this error should be much higher for fraudulent claims than non-fraudulent ones.\n",
    "\n",
    "3. Assess predictions of autoencoder created\n",
    "\n",
    "   - Use your trained autoencoder to predict the test set and define the corresponding losses(?). \n",
    "   - Create a histogram of your test set claims, clearly marking fraudulent and non- fraudulent claims. \n",
    "   - Discuss how you could use this to decide whether a transaction is fraudulent or not. \n",
    "   - Can you also derive an AUC in this approach - if yes, how does it perform compared to the previous approaches?\n",
    "\n",
    "### Answer\n",
    "Before creating an autoencoder, the pre-processed data from **Question 2** was split into training, validation and test sets. The training set only contained non-fraud claims, whereas validation and test sets contained a mixture of both non-fraud and fraud claims such that the total fraud claims were equally distributed among the validation and test sets.\n",
    "\n",
    "The following is a step-by-step methodology for splitting the original (pre-processed) dataset:\n",
    "1. Split pre-processed dataset into non-fraud and fraud dataframes\n",
    "2. Calculate the non-fraud and fraud sample sizes for training, validation and test sets respectively according to `train_split` percentage (this percentage does not include the proportion for validation set). The percentage splits for validation and test sets are equally divided after subtracting `train_split` percentage.\n",
    "3. Sample non-fraud data for training set first, then sample non-fraud and fraud data for validation and test sets respectively.\n",
    "\n",
    "After splitting, `scaler.fit_transform()` was applied to both training and validation sets and `scaler.transform()` was applied to the test set to ensure that our test set is truly unseen (i.e. the model does not learn about the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a copy of raw df\n",
    "\n",
    "df_autoencoder = df.copy()\n",
    "# split dataset into non-fraud (normal) and fraud\n",
    "normal_df = df_autoencoder[df_autoencoder.Fraud == 0]\n",
    "fraud_df = df_autoencoder[df_autoencoder.Fraud == 1]\n",
    "print(f'The length of normal_df: {len(normal_df)}')\n",
    "print(f'The length of fraud_df: {len(fraud_df)}')\n",
    "\n",
    "# variables for splitting data into train, val and test sets \n",
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "normal_train_size = round(len(df_autoencoder) * train_split)\n",
    "fraud_val_test_size = int(len(fraud_df) / 2)\n",
    "normal_val_test_size = int(round(len(df_autoencoder) * test_split / 2) - fraud_val_test_size)\n",
    "print(f'normal_train_size: {normal_train_size}')\n",
    "print(f'fraud_val_test_size: {fraud_val_test_size}')\n",
    "print(f'normal_val_test_size: {normal_val_test_size}\\n')\n",
    "\n",
    "# sample non-fraud data for train set\n",
    "train_df = normal_df.sample(normal_train_size, random_state=0)\n",
    "normal_df = normal_df[~normal_df.isin(train_df)].dropna()\n",
    "print(f'len(train_df): {len(train_df)}')\n",
    "print(f'len(normal_df) excluding data in train_df: {len(normal_df)}\\n')\n",
    "\n",
    "# sample non-fraud data for val and test sets\n",
    "val_df = normal_df.sample(normal_val_test_size, random_state=0)\n",
    "test_df = normal_df[~normal_df.isin(val_df)].dropna()\n",
    "print(f'len(val_df): {len(val_df)}')\n",
    "print(f'len(test_df): {len(test_df)}\\n')\n",
    "\n",
    "# check if all normal data is in the train, val and test sets\n",
    "normal_df = df_autoencoder[df_autoencoder.Fraud == 0]\n",
    "test = pd.concat([train_df, val_df, test_df]).isin(normal_df)\n",
    "print(f'len(test[test.LossDate == False]) = {len(test[test.LossDate == False])}\\n')\n",
    "\n",
    "# sample fraud data for val and test sets\n",
    "val_df = val_df.append(fraud_df.sample(fraud_val_test_size, random_state=0))\n",
    "test_df = test_df.append(fraud_df[~fraud_df.isin(val_df)].dropna())\n",
    "print(f'len(val_df): {len(val_df)}')\n",
    "print(f'len(test_df): {len(test_df)}\\n')\n",
    "\n",
    "print(f'Check if len(train_df) + len(val_df) + len(test_df) == len(df_autoencoder): {len(train_df) + len(val_df) + len(test_df) == len(df_autoencoder)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dataframe into numpy arrays\n",
    "y_train = train_df[['Fraud']].to_numpy()\n",
    "y_val = val_df[['Fraud']].to_numpy()\n",
    "y_test = test_df[['Fraud']].to_numpy()\n",
    "\n",
    "X_train = train_df.drop(['Fraud'], axis=1)\n",
    "X_val = val_df.drop(['Fraud'], axis=1)\n",
    "X_test = test_df.drop(['Fraud'], axis=1)\n",
    "\n",
    "train_col_names = list(X_train.columns)+['df_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train.loc[:,'df_key'] = 1\n",
    "X_val.loc[:,'df_key'] = 0\n",
    "X_test.loc[:,'df_key'] = 0\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "# X_df_key = X_train_val[['df_key']]\n",
    "# X_train_val = X_train_val.drop(['df_key'], axis=1)\n",
    "\n",
    "X_train_val = pd.DataFrame(scaler.fit_transform(X_train_val), columns=train_col_names)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=train_col_names)\n",
    "X_test = X_test.drop(['df_key'], axis=1)\n",
    "X_train = X_train_val[X_train_val.df_key == 1].drop(['df_key'], axis=1)\n",
    "X_val = X_train_val[X_train_val.df_key == 0].drop(['df_key'], axis=1)\n",
    "\n",
    "print(f'check if len(X_train) + len(X_val) + len(X_test) == len(df_autoencoder): {len(X_train) + len(X_val) + len(X_test) == len(df_autoencoder)}')\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_val.shape: {X_val.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "### Question\n",
    "Using TensorFlow, create an autoencoder, ensuring that the middle hidden layer has fewer neurons than your input has features. Use training and validation sets to find a model that represents its input data well. In particular, you will want to predict your validation set observations. For each observation, you can measure the difference between the original observations and the predicted one, using, for example, the mean squared error of all features of the observation. Plot the errors for all your validation set observations in a histogram - in a good model, this error should be much higher for fraudulent claims than non-fraudulent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Create an autoencoder\n",
    "During the optimization of a basic autoencoder, we observed that it was difficult to assess whether the created autoencoder was overfitting since the train set is exclusively non-fraud, whereas the validation set is a mixture of non-fraud and fraud. We cannot conclude whether the “val_loss” is associated with an overfitting issue or the model failed to identify the pattern of fraud features, or both. To overcome this, we further split the main training set containing only non-fraud data into `X_train_train` set (70%) and `X_train_val` set (30%) to assess overfitting using the difference between final “val_loss” and final “loss” (from the last epoch). If the difference is very small, it means the model is not overfitting and can reconstruct non-fraud features well. We represented this difference as `overfit_metric` and aim to find the minimum.\n",
    "\n",
    "After ensuring that the model is not overfitting on the `X_train_val` set, we continued to evaluate the trained model on the `X_val` set (the original validation set containing a mixture of non-fraud and fraud). If the model is good at reconstructing non-fraud features only, the MSE for non-fraud claims would be very close to zero, and the majority error would be associated with fraud claims. Therefore, we take the average of all MSE from the reconstruction predictions as a metric to assess whether the model reconstructs fraud features badly. We named this metric as `val2_avg_recon_error` and aim to find the maximum.\n",
    "\n",
    "With these 2 metrics, we developed a `model_score` by taking the difference between `val2_avg_recon_error` and `overfit_metric` to select the best model from a list of models where the following hyperparameters were tuned:\n",
    "   - dropout rate\n",
    "   - L2 regularisation parameter\n",
    "   - number of neurons\n",
    "   - number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train,X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncodingModel:\n",
    "    def __init__(self, reg_param, number_units_layers, number_units_bottleneck, dropout_rate, number_layers, whether_dropout, whether_regularizer, X_train_train=X_train_train, X_train_val=X_train_val, X_val=X_val,epochs=100) -> None:\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(192)\n",
    "        self.X_train_train = X_train_train\n",
    "        self.X_train_val = X_train_val\n",
    "        self.X_val = X_val\n",
    "        self.reg_param = reg_param\n",
    "        self.number_units_layers = number_units_layers\n",
    "        self.number_units_bottleneck = number_units_bottleneck\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.number_layers = number_layers\n",
    "        self.whether_dropout =  whether_dropout\n",
    "        self.whether_regularizer = whether_regularizer\n",
    "        self.input_dim = self.X_train_train.shape[1]\n",
    "        self.epochs  = epochs\n",
    "        self.build_model()\n",
    "        self.compile_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the model according to the hyperparameters input\n",
    "        \"\"\"\n",
    "        regularizer = tf.keras.regularizers.l2(\n",
    "            self.reg_param*self.whether_regularizer)\n",
    "        if self.whether_dropout == True:\n",
    "            encoder = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(self.number_units_layers, activation=\"relu\")]*self.number_layers+[\n",
    "                tf.keras.layers.Dropout(self.dropout_rate), # dropout before the bottleneck layer\n",
    "                tf.keras.layers.Dense(self.number_units_bottleneck,activation='sigmoid', kernel_regularizer=regularizer)])\n",
    "        else:\n",
    "            encoder = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(self.number_units_layers, activation=\"relu\")]*self.number_layers+[\n",
    "                tf.keras.layers.Dense(self.input_dim,activation='sigmoid')])\n",
    "        decoder = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(self.number_units_layers, activation=\"relu\")]*self.number_layers+[\n",
    "                tf.keras.layers.Dense(self.input_dim, activation=\"sigmoid\")])\n",
    "        self.autoencoder = tf.keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "        # random_seed = 192\n",
    "    def get_hp(self):\n",
    "        # get all hyperparameters\n",
    "        return {\n",
    "            'reg_param': self.reg_param,\n",
    "            'number_units_layers': self.number_units_layers,\n",
    "            'number_units_bottleneck': self.number_units_bottleneck,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'number_layers': self.number_layers,\n",
    "            'whether_dropout': self.whether_dropout,\n",
    "            'whether_regularizer': self.whether_regularizer\n",
    "        }\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        get the model from the class\n",
    "        \"\"\"\n",
    "        return self.autoencoder\n",
    "\n",
    "    def compile_model(self):\n",
    "        \"\"\"\n",
    "        compile the model\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.autoencoder.compile(\n",
    "            optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    def __train_model(self):\n",
    "        \"\"\"\n",
    "        Train the data only contains non-fraud claims\n",
    "        \"\"\"\n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.log_train_non_fraud = self.autoencoder.fit(x=self.X_train_train, y=self.X_train_train,\n",
    "                                                        epochs=self.epochs,\n",
    "                                                        validation_data=(X_train_val, X_train_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "    def get_train_non_fraud_loss_diff(self):\n",
    "        \"\"\"\n",
    "        calculate the overfit_metric\n",
    "        \"\"\"\n",
    "        self.__train_model()\n",
    "        return self.log_train_non_fraud.history['val_loss'][-1]-self.log_train_non_fraud.history['loss'][-1] \n",
    "        \n",
    "    def get_train_val_loss(self):\n",
    "        \"\"\"\n",
    "        get the val1_loss\n",
    "        \"\"\"\n",
    "        return self.log_train_non_fraud.history['val_loss'][-1]\n",
    "\n",
    "    def __apply_model_fraud(self):\n",
    "        \"\"\"\n",
    "        apply the model on the val2, and get the overall average mse loss\n",
    "        \"\"\"\n",
    "        reconstructions = self.autoencoder.predict(self.X_val)\n",
    "        self.val_loss = np.mean(tf.keras.losses.mse(reconstructions, self.X_val))\n",
    "\n",
    "    def get_val_loss(self):\n",
    "        self.__apply_model_fraud()\n",
    "        return self.val_loss\n",
    "\n",
    "    def run(self):\n",
    "        # the difference of loss in the train_train and train_val, the metric to access the overfitting\n",
    "        overfit_metric = self.get_train_non_fraud_loss_diff()  #TODO change it to overfit_metric\n",
    "        val2_avg_recon_error = self.get_val_loss() #TODO change it to val2_avg_recon_error\n",
    "        val1_loss = self.get_train_val_loss()  #TODO change it to val1_loss\n",
    "        return {\"overfit_metric\": overfit_metric, \"val2_avg_recon_error\": val2_avg_recon_error, \"val1_loss\":val1_loss, \"model\": self.autoencoder, \"log\":self.log_train_non_fraud}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of hyperparameters and train the model. After trainnig the model, we will record the hyperparameters and the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(192)\n",
    "round = 30\n",
    "model_list = [] # list of to save model\n",
    "class_list = []\n",
    "log_list = []\n",
    "para_dfm = pd.DataFrame(columns=['reg_param', 'number_units_layers', 'number_units_bottleneck', 'dropout_rate', 'number_layers', 'whether_dropout', 'whether_regularizer','overfit_metric', 'val1_loss','val2_avg_recon_error'])\n",
    "for i in range(round):\n",
    "    print(f\"This round is {i}\")\n",
    "    # generate a list of hyperparameters\n",
    "    reg_param = np.random.uniform(low=0.1, high=0.3)\n",
    "    numbers_units_layers = np.random.choice(np.arange(30, 61, 15))\n",
    "    numbers_units_bottleneck = np.random.choice(np.arange(5, 16, 5))\n",
    "    dropout_rate = np.random.uniform(low=0.01, high=0.05)\n",
    "    numbers_layers = np.random.choice(np.arange(1, 4, 1))\n",
    "    whether_dropout = np.random.choice([True, False])\n",
    "    whether_regularizer = np.random.choice([True, False])\n",
    "    autoencoder = AutoEncodingModel(reg_param, numbers_units_layers, numbers_units_bottleneck, dropout_rate, numbers_layers, whether_dropout, whether_regularizer)\n",
    "    \n",
    "    # get the result of the model\n",
    "    res = autoencoder.run()\n",
    "    overfit_metric  = res['overfit_metric']\n",
    "    val2_avg_recon_error = res['val2_avg_recon_error']\n",
    "    model = res['model']\n",
    "    val1_loss = res['val1_loss']\n",
    "\n",
    "    # fill the value into the dataframe\n",
    "    para_dfm = para_dfm.append({\"reg_param\": reg_param, \"number_units_layers\": numbers_units_layers, \"number_units_bottleneck\": numbers_units_bottleneck, \"dropout_rate\": dropout_rate, \"number_layers\": numbers_layers, \"whether_dropout\": whether_dropout, \"whether_regularizer\": whether_regularizer, \"overfit_metric\": overfit_metric, \"val2_avg_recon_error\": val2_avg_recon_error,'val1_loss':val1_loss}, ignore_index=True)\n",
    "    \n",
    "    # add model into model_list\n",
    "    model_list.append(model)\n",
    "\n",
    "    # add initalized class into class_list\n",
    "    class_list.append(autoencoder)\n",
    "\n",
    "    # add log into log_list\n",
    "    log_list.append(res['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_dfm.to_csv(\"para_dfm.csv\")\n",
    "para_dfm = pd.read_csv(\"para_dfm.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the model score according to the val2_avg_recon_error and overfit_metric\n",
    "para_dfm[\"model_score\"] = para_dfm[\"val2_avg_recon_error\"] - para_dfm[\"overfit_metric\"] \n",
    "para_dfm = para_dfm.sort_values(by=[\"model_score\"], ascending=False)\n",
    "para_dfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the models we have trained and look into the relationship of the model_score and the performance of model.\n",
    "In the table, we are going to record the model_score, the sensitivity(select the mean of reconstruction_error as threshold), accuracy_rate and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_performance_diff_sens.to_csv(\"df_performance_diff_sens.csv\")\n",
    "df_performance_diff_sens = pd.read_csv(\"df_performance_diff_sens.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: read \"df_performance_diff_sens\" from local file\n",
    "\n",
    "# df_performance_diff_sens = pd.DataFrame(columns=['model_score', 'sensitivity','accuracy_rate','roc'])\n",
    "# for ind, row in para_dfm.iterrows():\n",
    "#     print(f\"The model_score is: {row['model_score']}\")\n",
    "#     model_pred = model_list[ind]\n",
    "#     X_pred = model_pred.predict(X_val)\n",
    "#     mse = np.mean(np.power(X_val.flatten() - X_pred.flatten(), 2))\n",
    "#     reconstructions = model_pred.predict(X_val)\n",
    "#     val_loss = tf.keras.losses.mae(reconstructions, X_val)\n",
    "#     # sns.histplot(x=val_loss,y=y_val.flatten(),hue=y_val.flatten())\n",
    "#     # plt.show()\n",
    "#     df_tmp = pd.DataFrame({\"val_loss\": val_loss, \"y_val\": y_val.flatten()})\n",
    "#     df_tmp_fraud = df_tmp[df_tmp[\"y_val\"] == 1]\n",
    "#     df_tmp_non_fraud = df_tmp[df_tmp[\"y_val\"] == 0]\n",
    "#     mse = np.mean(np.power(X_val.flatten() - X_pred.flatten(), 2))\n",
    "#     error_df = pd.DataFrame({'Reconstruction_error': mse, 'True_class': y_val.flatten()})\n",
    "#     df_temp = pd.DataFrame({'Reconstruction_error': val_loss, 'True_class': y_val.flatten()})\n",
    "#     roc = roc_auc_score(y_val.flatten(), val_loss)\n",
    "#     threshold = np.mean(df_temp[\"Reconstruction_error\"])\n",
    "#     pred_y = np.where(val_loss > threshold, 1, 0)\n",
    "#     cm = confusion_matrix(y_val.flatten(), pred_y)\n",
    "#     accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "#     # calculate the sensitivity \n",
    "#     sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "#     print(f\"sensitivity is: {sensitivity}\")\n",
    "#     df_performance_diff_sens = df_performance_diff_sens.append({\"model_score\": row['model_score'], \"sensitivity\": sensitivity,'accuracy_rate':accuracy_rate,'roc':roc}, ignore_index=True)\n",
    "\n",
    "# df_performance_diff_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Assess autoencoder on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore the distubution of the fraud cases in the test set of each model. Usually, the higher the model score, the fraud cases are more likely to be concentrated at a large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in para_dfm.iterrows():\n",
    "    print(f\"The model_score is: {row['model_score']}\")\n",
    "    model_pred = model_list[ind]\n",
    "    X_pred = model_pred.predict(X_val)\n",
    "    mse = np.mean(np.power(X_val.flatten() - X_pred.flatten(), 2))\n",
    "    reconstructions = model_pred.predict(X_val)\n",
    "    val_loss = tf.keras.losses.mse(reconstructions, X_val)\n",
    "    df_tmp = pd.DataFrame({\"val_loss\": val_loss, \"y_val\": y_val.flatten()})\n",
    "    df_tmp_fraud = df_tmp[df_tmp[\"y_val\"] == 1]\n",
    "    df_tmp_non_fraud = df_tmp[df_tmp[\"y_val\"] == 0]\n",
    "    plt.hist(df_tmp_fraud[\"val_loss\"], bins=50, alpha=0.5, label=\"fraud\", color=\"red\",density=True)\n",
    "    plt.hist(df_tmp_non_fraud[\"val_loss\"], bins=50, alpha=0.5, label=\"non_fraud\", color=\"blue\",density=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the relationship between model_score and other columns, we can find a very significant linear relationship between these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_diff_sens.plot(x='model_score', y='sensitivity', kind='scatter',title='Sensitivity_model_score')\n",
    "plt.show()\n",
    "df_performance_diff_sens.plot(x='model_score', y='roc', kind='scatter',title='ROC_model_score')\n",
    "plt.show()\n",
    "df_performance_diff_sens.plot(x='model_score', y='accuracy_rate', kind='scatter',title='Accuracy_model_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use statsmodel to test whether it is significant. We can find the p-value of the model is very low, which means the model is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression #TODO more linear regression test\n",
    "\n",
    "X  = np.array(df_performance_diff_sens['model_score'], dtype=float)\n",
    "y = np.array(df_performance_diff_sens['sensitivity'], dtype=float)\n",
    "X = sm.add_constant(X)\n",
    "print(\"Statsmodels linear regression betwen model_score and sensitivity\")\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "X = np.array(df_performance_diff_sens['model_score'], dtype=float)\n",
    "y = np.array(df_performance_diff_sens['accuracy_rate'], dtype=float)\n",
    "X = sm.add_constant(X)\n",
    "print(\"Statsmodels linear regression betwen model_score and accuracy_rate\")\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "X = np.array(df_performance_diff_sens['model_score'], dtype=float)\n",
    "y = np.array(df_performance_diff_sens['roc'], dtype=float)\n",
    "X = sm.add_constant(X)\n",
    "print(\"Statsmodels linear regression betwen model_score and roc\")\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "print('-------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "\n",
    "### Question\n",
    "Use your trained autoencoder to predict the test set and define the corresponding losses. Create a histogram of your test set claims, clearly marking fraudulent and nonfraudulent claims. Discuss how you could use this to decide whether a transaction is fraudulent or not. Can you also derive an AUC in this approach - if yes, how does it perform compared to the previous approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Assess autoencoder on test set\n",
    "\n",
    "Based on the results of the previous question, we can use the model_score to select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of largest model_score row\n",
    "para_dfm_max_index = para_dfm[para_dfm[\"model_score\"]==max(para_dfm[\"model_score\"])].index[0]\n",
    "# get the loss function of the best model\n",
    "def create_plot(log,limit=None):\n",
    "    if limit:\n",
    "    # plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "        plt.plot(log.history['loss'][-limit:],label = \"training loss\",color='darkgreen')\n",
    "        # plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "        plt.plot(log.history['val_loss'][-limit:], label = \"validation loss\",color='darkblue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "        plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "create_plot(log_list[para_dfm_max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of largest model_score row\n",
    "autoencoder_best = model_list[int(para_dfm_max_index)]\n",
    "# autoencoder_best.save('./models/autoencoder_best.h5') # save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to reset the weight and train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# reset the all the weight of autoencoder model\n",
    "autoencoder_best = shuffle_weights(autoencoder_best)\n",
    "log = autoencoder_best.fit(x=X_train, y=X_train, epochs=200, validation_data=(X_val, X_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_best.save('models/autoencoder_best.h5')\n",
    "autoencoder_best = tf.keras.models.load_model('./models/autoencoder_best.h5')\n",
    "X_pred = autoencoder_best.predict(X_val)\n",
    "mse = np.mean(np.power(X_val.flatten() - X_pred.flatten(), 2))\n",
    "reconstructions = autoencoder_best.predict(X_val)\n",
    "val_loss = tf.keras.losses.mae(reconstructions, X_val)\n",
    "# sns.histplot(x=val_loss,y=y_val.flatten(),hue=y_val.flatten())\n",
    "# plt.show()\n",
    "df_tmp = pd.DataFrame({\"val_loss\": val_loss, \"y_val\": y_val.flatten()})\n",
    "df_tmp_fraud = df_tmp[df_tmp[\"y_val\"] == 1]\n",
    "df_tmp_non_fraud = df_tmp[df_tmp[\"y_val\"] == 0]\n",
    "mse = np.mean(np.power(X_val.flatten() - X_pred.flatten(), 2))\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse, 'True_class': y_val.flatten()})\n",
    "df_temp = pd.DataFrame({'Reconstruction_error': val_loss, 'True_class': y_val.flatten()})\n",
    "roc = roc_auc_score(y_val.flatten(), val_loss)\n",
    "print(\"ROC: \", roc)\n",
    "#plot the roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val.flatten(), val_loss)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the threshold for fraud\n",
    "We are going to use three method to set the threshold for fraud:\n",
    "1. Use the mean of the reconstruction error plus one standard deviation as the threshold\n",
    "2. Set the threshold to the minimum of the reconstruction error of fraud cases\n",
    "3. Set the threshold to the minimize the cost function we mentioned above, which is 10*FN + FP\n",
    "\n",
    "If it is real case, we would recommend to use the third method. Because it can have company to lower the cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "Use the mean of the reconstruction error plus one standard deviation as the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the threshold with test dataset\n",
    "threshold = np.mean(val_loss) + np.std(val_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred y based on the threshold\n",
    "test_loss = autoencoder_best.predict(X_test)\n",
    "reconstructions = autoencoder_best.predict(X_val)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, X_val)\n",
    "pred_y = np.where(test_loss > threshold, 1, 0)\n",
    "cm = confusion_matrix(y_test.flatten().astype(int), pred_y)\n",
    "print(f\"Confusion matrix is :\" )\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.show()\n",
    "accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "print(f\"Accuracy rate is {accuracy_rate}\")\n",
    "# calculate the sensitivity \n",
    "sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "print(f\"Sensitivity is {sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "Set the threshold to the minimum of the reconstruction error of fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of the model_score\n",
    "\n",
    "sns.histplot(x=val_loss,y=y_val.flatten(),hue=y_val.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the threshold with test dataset\n",
    "df_temp = pd.DataFrame({'Reconstruction_error': val_loss, 'True_class': y_val.flatten()})\n",
    "df_temp_fraud = df_temp[df_temp['True_class']==1]\n",
    "threshold = df_temp_fraud[\"Reconstruction_error\"].min()\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred y based on the threshold\n",
    "pred_y = np.where(test_loss > threshold, 1, 0)\n",
    "cm = confusion_matrix(y_test.flatten().astype(int), pred_y)\n",
    "print(f\"Confusion matrix is :\" )\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "print(f\"Accuracy rate is {accuracy_rate}\")\n",
    "# calculate the sensitivity \n",
    "sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "print(f\"Sensitivity is {sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3\n",
    "Set the threshold to the minimize the cost function we mentioned above, which is 10*FN + FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the threshold with test dataset\n",
    "#  the cost function\n",
    "def calculate_cost(fn,fp):\n",
    "    return 100*fn+fp\n",
    "\n",
    "cost_lost = {}\n",
    "for i in np.linspace(0,0.1,100):\n",
    "    pred_y = np.where(val_loss > i, 1, 0)\n",
    "    cm = confusion_matrix(y_val.flatten(), pred_y)\n",
    "    fn,fp = cm[1][0],cm[0][1]\n",
    "    # cost_lost[\"Threshold: \"+str(i)] = calculate_cost(fn,fp)\n",
    "    cost_lost[i] = calculate_cost(fn,fp)\n",
    "\n",
    "threshold = min(cost_lost, key=cost_lost.get)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred y based on the threshold\n",
    "pred_y = np.where(test_loss > threshold, 1, 0)\n",
    "cm = confusion_matrix(y_test.flatten().astype(int), pred_y)\n",
    "print(f\"Confusion matrix is :\" )\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy_rate = (cm[0,0] + cm[1,1])/np.sum(cm)\n",
    "print(f\"Accuracy rate is {accuracy_rate}\")\n",
    "# calculate the sensitivity \n",
    "sensitivity = cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "print(f\"Sensitivity is {sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Conclusion\n",
    "\n",
    "We can obtain a relatively higher ROC score using the autoencoder for anomaly detection. Which means we can balance the sensitity and specificity well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "As you know, it is difficult to understand precisely why a neural network makes a specific prediction. Discuss why this might be problematic when the neural network prediction leads to a fraud investigation by the insurance company. What alternatives can you envision that make use of the techniques we have applied and allow for more interpretability and transparency?\n",
    "\n",
    "### Answer\n",
    "The issue with neural network predictions in a fraud investigation is its “black box” behaviour: its internal logic in achieving the classification results is often hard to be interpreted or explained to both users and observers. If a customer is wrongly investigated for fraud by the company, they would generally demand an understanding of why they were investigated. Due to its complex intricacies, neural networks fail to explain why a claim is fraudulent despite its potentially high accuracy.\n",
    "\n",
    "Solutions to neural network anomaly detection can be recommended in two ways. Firstly the model can be improved and refined to continuously perform better after each iterative improvement. Secondly, alternative machine learning classifications can replace the neural networks to provide greater context to the decision to investigate a customer for potential fraud. Regarding the first point, several alternative neural network classification models can be suggested particularly for anomaly detection. Al’Dahoul et al., 2021 created a novel neural network strategy that “combines binary normal/attack DNN to detect the availability of any attack and multi-attacks DNN to categorize the attacks” (Al’Dahoul et al., 2021:16). This anomaly detection and classification model outperformed the baseline solution and reduced the ‘false alarm rate’ for the highly imbalanced dataset. Additionally, Long Short-Term Memory (LSTM) neural networks can be implemented to find complex relationships in multivariate time series data. This system looks at the previous timeframe and predicts the behavior for the next. If the actual value a minute later is within one standard deviation then there is no problem. Other novel classification solutions include DAICS, which is more robust to the additive noise (Abdelaty et al., 2020), GT, which eliminates the need for autoencoders and improves results (Golan & Yaniv, 2018), and E3 outlier, which uses inlier priority for unsupervised outlier detection  (Wang et al., 2019). These solutions are more accurate, and improve results such as ROC values of the models. \n",
    "\n",
    "\n",
    "There are alternative machine learning classification models that can both make use of the techniques we have applied, and also allow a much better interpretability and transparency. Decision trees, for example, can explain the features that are used to segment a claim to fraud and non-fraud, while random forests can be used to show the importance of each feature in classifying the claims. This is a better improvement for transparency, as the features can highlight the information that can potentially be linked to fraudulent behaviours. Furthermore, both random forests and decision trees can be tuned for the right hyperparameters in order to optimise the model. Decision trees can be pruned, while random forest can have hyperparameters, such as maximum number of depths, number of trees and the minimum number of samples per leaf, be fully optimised. Additionally, random forests can provide a different interpretation of a decision tree but with better performance. Considering we need to understand which combination of variables flagged a customer for investigation, a decision tree can help understand how each variable is contributing to the prediction model, despite providing a reduction in the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "\n",
    "### Question\n",
    "\n",
    "Use your synthetically extended dataset and train a simple model, such as logistic\n",
    "regression or a decision tree that allows you to interpret why fraud is suspected. Keep track of the accuracy and AUC on a test set made from original data only. How does your model perform compared to the previous models you have developed? Does your model allow you to answer a customer who asks, \"why am I being investigated\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building 3 simple models for our enhanced dataset\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def plot_confusionmatrix(y_train_pred,y_train,dom):\n",
    "    print(f'{dom} Confusion matrix')\n",
    "    cf = confusion_matrix(y_train_pred,y_train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=classes\n",
    "               ,xticklabels=classes,cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Predicting the score\n",
    "\n",
    "def ModelPerformance(model, X_test, y_test):\n",
    "    predlr = model.predict(X_test)\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(y_test, predlr)*100))\n",
    "    print(\"f1_Score: {}\".format(f1_score(y_test, predlr)*100))\n",
    "    print(confusion_matrix(y_test, predlr))\n",
    "\n",
    "def Model_ROC_AUC(model, x, y):\n",
    "    Y_probs=model.predict_proba(x)[:,1]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y,Y_probs)\n",
    "    plt.plot(fpr, tpr, linewidth=4)\n",
    "    plt.show()\n",
    "    print(f'AUC score: {roc_auc_score(y,Y_probs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_over_synth_10_train\n",
    "y_train = y_over_synth_10_train\n",
    "x_val = X_over_synth_10_val\n",
    "y_val = y_over_synth_10_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for LogisticRegression (basic model)\n",
    "\n",
    "def LogisticRegressionModel(x_train, y_train):\n",
    "    lr = LogisticRegression(max_iter=300)\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionModel(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------\")\n",
    "print(\"Accuracy of Logistic Regression on Test Dataset\")\n",
    "print(\"---------------------------\")\n",
    "ModelPerformance(lr, X_test, y_test)\n",
    "Model_ROC_AUC(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a simple decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(random_state=66)\n",
    "dtc.fit(x_train,y_train)\n",
    "y_train_pred = dtc.predict(x_train)\n",
    "y_val_pred = dtc.predict(x_val)\n",
    "y_test_pred = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T05:17:43.672728Z",
     "iopub.status.busy": "2021-06-22T05:17:43.671783Z",
     "iopub.status.idle": "2021-06-22T05:17:43.684329Z",
     "shell.execute_reply": "2021-06-22T05:17:43.684836Z"
    },
    "papermill": {
     "duration": 0.049985,
     "end_time": "2021-06-22T05:17:43.684988",
     "exception": false,
     "start_time": "2021-06-22T05:17:43.635003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = dtc.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T05:17:43.760363Z",
     "iopub.status.busy": "2021-06-22T05:17:43.759265Z",
     "iopub.status.idle": "2021-06-22T05:17:43.835519Z",
     "shell.execute_reply": "2021-06-22T05:17:43.835996Z"
    },
    "papermill": {
     "duration": 0.11391,
     "end_time": "2021-06-22T05:17:43.836160",
     "exception": false,
     "start_time": "2021-06-22T05:17:43.722250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each alpha we will append our model to a list\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in clfs:\n",
    "    y_train_pred = c.predict(x_train)\n",
    "    y_test_pred = c.predict(x_val)\n",
    "    train_acc.append(accuracy_score(y_train_pred,y_train))\n",
    "    test_acc.append(accuracy_score(y_test_pred,y_val))\n",
    "\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,test_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,test_acc,label='val_accuracy',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036937,
     "end_time": "2021-06-22T05:17:44.629496",
     "exception": false,
     "start_time": "2021-06-22T05:17:44.592559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We choose the alpha value to be 0.003 as our hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T05:17:44.704955Z",
     "iopub.status.busy": "2021-06-22T05:17:44.704305Z",
     "iopub.status.idle": "2021-06-22T05:17:45.079832Z",
     "shell.execute_reply": "2021-06-22T05:17:45.079134Z"
    },
    "papermill": {
     "duration": 0.414528,
     "end_time": "2021-06-22T05:17:45.079949",
     "exception": false,
     "start_time": "2021-06-22T05:17:44.665421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_ = tree.DecisionTreeClassifier(random_state=66,ccp_alpha=0.003)\n",
    "clf_.fit(x_train,y_train)\n",
    "y_train_pred = clf_.predict(x_train)\n",
    "y_val_pred = clf_.predict(x_val)\n",
    "y_test_pred = clf_.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T05:17:45.249533Z",
     "iopub.status.busy": "2021-06-22T05:17:45.244311Z",
     "iopub.status.idle": "2021-06-22T05:17:45.691549Z",
     "shell.execute_reply": "2021-06-22T05:17:45.690416Z"
    },
    "papermill": {
     "duration": 0.493461,
     "end_time": "2021-06-22T05:17:45.691686",
     "exception": false,
     "start_time": "2021-06-22T05:17:45.198225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "features = df.columns\n",
    "classes = ['Not Fraud','Fraud']\n",
    "tree.plot_tree(clf_,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------\")\n",
    "print(\"Accuracy of Decision Tree on Test Dataset\")\n",
    "print(\"---------------------------\")\n",
    "ModelPerformance(clf_, X_test, y_test)\n",
    "Model_ROC_AUC(clf_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a simple Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=66)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rfc.predict(x_train)\n",
    "y_val_pred = rfc.predict(x_val)\n",
    "y_test_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using Randomized Search Cross Validation\n",
    "\n",
    "To potentially avoid the issue of overfitting, we try to optimise our random forest model through randomized search cross validation in order to tune our hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] # number of trees in the random forest\n",
    "max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
    "max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\n",
    "min_samples_split = [2, 5, 10] # minimum sample number to split a node\n",
    "min_samples_leaf = [1, 2, 4] # minimum sample number that can be stored in a leaf node\n",
    "bootstrap = [True, False] # method used to sample data points\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "'max_features': max_features,\n",
    "\n",
    "'max_depth': max_depth,\n",
    "\n",
    "'min_samples_split': min_samples_split,\n",
    "\n",
    "'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T14:47:53.716594Z",
     "iopub.status.busy": "2021-08-18T14:47:53.716112Z",
     "iopub.status.idle": "2021-08-18T14:47:53.729015Z",
     "shell.execute_reply": "2021-08-18T14:47:53.727958Z",
     "shell.execute_reply.started": "2021-08-18T14:47:53.716552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator = rfc,param_distributions = random_grid,\n",
    "               n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T14:47:53.730608Z",
     "iopub.status.busy": "2021-08-18T14:47:53.730306Z",
     "iopub.status.idle": "2021-08-18T14:59:18.766419Z",
     "shell.execute_reply": "2021-08-18T14:59:18.765123Z",
     "shell.execute_reply.started": "2021-08-18T14:47:53.730579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Random grid: ', random_grid, '\\n')\n",
    "# print the best parameters\n",
    "print ('Best Parameters: ', rf_random.best_params_, ' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the best parameters into the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned = RandomForestClassifier(n_estimators = 190, min_samples_split = 2, \n",
    "                                    min_samples_leaf= 1, max_features = 'auto', max_depth= 110, \n",
    "                                    bootstrap=False, random_state=66) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------\")\n",
    "print(\"Accuracy of Random Forest on Test Dataset\")\n",
    "print(\"---------------------------\")\n",
    "ModelPerformance(rfc_tuned, X_test, y_test)\n",
    "Model_ROC_AUC(rfc_tuned, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training logistic regression, decision tree and random forest on our enhanced model, we found the following AUC and accuracy score for our models:\n",
    "<br>   \n",
    "<br> \n",
    "__Logistic Regression__\n",
    "\n",
    "Accuracy = 97.0%\n",
    "\n",
    "AUC Score = 85.7%\n",
    "\n",
    "__Decision Tree__\n",
    "\n",
    "Accuracy = 97.5%\n",
    "\n",
    "AUC Score = 73.2%\n",
    "\n",
    "__Random Forest__\n",
    "\n",
    "Accuracy = 98.8%\n",
    "\n",
    "AUC Score = 85.4%<br>   \n",
    "<br> \n",
    "\n",
    "The machine learning models has high accuracy and AUC scores, and perform better than the previous models. However, it is predicting non-fraud cases well but not fraud cases. The tree-based classification models can help explain why claims are being marked as fraudulent. For example, a feature “ClaimWithoutIdentifiedThirdParty” are considered as one of the significant features that distinguish a claim into fraudulent, acting as one of the main features to split a sample to ‘Fraud’ and ‘Not Fraud’. This feature can be rationalised, where fraud claims are more likely to not have a third party. Furthermore, Random Forest models can also calculate the importance of each variables when determining the classification. This can highlight which features are significant in decidingn the binary class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "References:\n",
    "\n",
    "Abdelaty, M. F., Doriguzzi Corin, R., & Siracusa, D. (2021;2020;). DAICS: A deep learning solution for anomaly detection in industrial control systems. IEEE Transactions on Emerging Topics in Computing, , 1-1. https://doi.org/10.1109/TETC.2021.3073017\n",
    "\n",
    "AlDahoul, N., Abdul Karim, H., & Ba Wazir, A. S. (2021). Model fusion of deep neural networks for anomaly detection. Journal of Big Data, 8(1), 1-18. https://doi.org/10.1186/s40537-021-00496-w\n",
    "\n",
    "Blanque, P. (2003). Crisis and fraud. Journal of Financial Regulation and Compliance, 11(1), 60-70. https://doi.org/10.1108/13581980310810417\n",
    "\n",
    "Chakravarty, S., Demirhan, H., & Baser, F. (2020). Fuzzy regression functions with a noise cluster and the impact of outliers on mainstream machine learning methods in the regression setting. Applied Soft Computing, 96, 106535. https://doi.org/10.1016/j.asoc.2020.106535\n",
    "\n",
    "Golan, I., & El-Yaniv, R. (2018). Deep anomaly detection using geometric transformations.\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\n",
    "\n",
    "Reed, R., & Marks, R. J. (1999). Neural smithing: Supervised learning in feedforward artificial neural networks. MIT Press. https://doi.org/10.7551/mitpress/4937.001.0001\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "627f9b2e5272ed3cf653ffd46ee5493b7b24a386103372a1710032d6985d4cd7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
